{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and actions\n",
    "data_path = \"action_data\"  # Main directory for data\n",
    "actions = [\"Walking\", \"Squat\", \"Crunching\", \"Alternate Toe Touch\", \"Jumping Jacks\", \"Right Shot\", \"Left Shot\"]  # Actions to collect data for\n",
    "threshold = 0.8  # Confidence threshold for action recognition\n",
    "motion_threshold = 0.2  # Threshold for minimum motion to classify as a valid action\n",
    "required_sequences = 30  # Expected number of sequences per action\n",
    "sequence_length = 20  # Length of each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media Pipe Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe setup\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # Pose landmarks (including legs)\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 3)\n",
    "    \n",
    "    # Left hand landmarks\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    \n",
    "    # Right hand landmarks\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    \n",
    "    # Left leg landmarks (indices 23-25 in Mediapipe)\n",
    "    left_leg = np.array([[res.x, res.y, res.z] for res in [results.pose_landmarks.landmark[i] for i in [23, 25, 27, 29, 31]]]).flatten() if results.pose_landmarks else np.zeros(5 * 3)\n",
    "    \n",
    "    # Right leg landmarks (indices 24-26 in Mediapipe)\n",
    "    right_leg = np.array([[res.x, res.y, res.z] for res in [results.pose_landmarks.landmark[i] for i in [24, 26, 28, 30, 32]]]).flatten() if results.pose_landmarks else np.zeros(5 * 3)\n",
    "    \n",
    "    # Concatenate all landmarks\n",
    "    return np.concatenate([pose, lh, rh, left_leg, right_leg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data verification and collection...\n",
      "Action 'Walking': 30/30 sequences available.\n",
      "Action 'Squat': 30/30 sequences available.\n",
      "Action 'Crunching': 30/30 sequences available.\n",
      "Action 'Alternate Toe Touch': 30/30 sequences available.\n",
      "Action 'Jumping Jacks': 30/30 sequences available.\n",
      "Action 'Right Shot': 30/30 sequences available.\n",
      "Action 'Left Shot': 30/30 sequences available.\n",
      "All resources released. Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Initialize resources\n",
    "cap = None\n",
    "holistic = None\n",
    "\n",
    "try:\n",
    "    # Initialize the Video Capture for live webcam\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "    # Set camera resolution to 1080p\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)  # Set width to 1920 pixels\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)  # Set height to 1080 pixels\n",
    "\n",
    "    # Initialize Mediapipe Holistic model\n",
    "    holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    print(\"Starting data verification and collection...\")\n",
    "\n",
    "    for action in actions:\n",
    "        folder_path = os.path.join(data_path, action)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Count existing sequences\n",
    "        existing_files = [file for file in os.listdir(folder_path) if file.endswith('.npy')]\n",
    "        num_files = len(existing_files)\n",
    "        print(f\"Action '{action}': {num_files}/{required_sequences} sequences available.\")\n",
    "\n",
    "        # Collect missing sequences\n",
    "        if num_files < required_sequences:\n",
    "            for sequence_num in range(num_files, required_sequences):\n",
    "                print(f\"Collecting sequence {sequence_num + 1} for '{action}'...\")\n",
    "                sequence = []\n",
    "\n",
    "                while len(sequence) < sequence_length:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        print(\"Camera error. Exiting...\")\n",
    "                        raise RuntimeError(\"Failed to read from camera.\")\n",
    "\n",
    "                    frame = cv2.flip(frame, 1)\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    results = holistic.process(frame_rgb)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Display progress information on the frame\n",
    "                    cv2.putText(frame, f\"Action: {action}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Sequence: {sequence_num + 1}/{required_sequences}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Frames: {len(sequence)}/{sequence_length}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                    # Show the frame\n",
    "                    cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "                    # Append keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    sequence.append(keypoints)\n",
    "\n",
    "                    # Exit on 'q' key\n",
    "                    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                        print(\"Data collection interrupted by user.\")\n",
    "                        raise KeyboardInterrupt  # Trigger safe exit\n",
    "\n",
    "                # Save the sequence\n",
    "                np.save(os.path.join(folder_path, f\"{sequence_num}.npy\"), sequence)\n",
    "                print(f\"Sequence {sequence_num + 1} saved.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOperation stopped by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Ensure resources are released\n",
    "    if cap:\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if holistic:\n",
    "        holistic.close()\n",
    "    print(\"All resources released. Exiting...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape verification complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "for action in actions:\n",
    "    folder_path = os.path.join(data_path, action)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = np.load(file_path)\n",
    "        if data.shape != (sequence_length, 255):  # Update 255 to match your feature size\n",
    "            print(f\"Inconsistent shape in {file_path}: {data.shape}\")\n",
    "print(\"Shape verification complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed sequence lengths.\n"
     ]
    }
   ],
   "source": [
    "def fix_sequence_length(data, target_length=30):\n",
    "    if len(data) > target_length:\n",
    "        return data[:target_length]  # Truncate\n",
    "    elif len(data) < target_length:\n",
    "        padding = np.zeros((target_length - len(data), data.shape[1]))  # Pad with zeros\n",
    "        return np.vstack((data, padding))  # Append padding\n",
    "    return data\n",
    "\n",
    "for action in actions:\n",
    "    folder_path = os.path.join(data_path, action)\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = np.load(file_path)\n",
    "        fixed_data = fix_sequence_length(data, target_length=sequence_length)\n",
    "        np.save(file_path, fixed_data)  # Overwrite with fixed sequence\n",
    "print(\"Fixed sequence lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (147, 20, 255), Validation set: (31, 20, 255), Test set: (32, 20, 255)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = [], []\n",
    "for action in actions:\n",
    "    for file in os.listdir(os.path.join(data_path, action)):\n",
    "        data = np.load(os.path.join(data_path, action, file))  # Load the .npy file\n",
    "        X.append(data)\n",
    "        y.append(label_map[action])  # Map action label to numeric value\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y).astype(int)  # Convert labels to one-hot encoding\n",
    "\n",
    "# First split: Train (70%) and Temp (30% for validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: Validation (15%) and Test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitted Folder Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully in the folder: data_splits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Specify the folder to save the datasets\n",
    "save_folder = \"data_splits\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Save datasets to the specified folder\n",
    "np.save(os.path.join(save_folder, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(save_folder, \"X_val.npy\"), X_val)\n",
    "np.save(os.path.join(save_folder, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(save_folder, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(save_folder, \"y_val.npy\"), y_val)\n",
    "np.save(os.path.join(save_folder, \"y_test.npy\"), y_test)\n",
    "\n",
    "print(f\"Datasets saved successfully in the folder: {save_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
